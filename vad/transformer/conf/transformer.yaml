# copied from asr tfu model
model: &model
  module: "mountaintop.models.vad.transformer:VadTransformer"
  base_config:
    vocab_size: &vocab_size !line_count [data/words.txt]
    embed_size: &embed_size 80
    hidden_size: &hidden_size 256
    dropout_rate: &dropout_rate 0.1
    positional_dropout_rate: &positional_dropout_rate 0.1
    attention_dropout_rate: 0.0

  embed: &embedding
    module: "mountaintop.layers.speech_embed:SpeechEmbed"
    in_dim: *embed_size
    out_dim: *hidden_size
    pos_type: "abs_pos"
    subsampling_type: "conv2d4" 
    dropout_rate: *dropout_rate
    positional_dropout_rate: *positional_dropout_rate 

  encoder: &encoder
    module: "mountaintop.layers.transformer.encoder:TransformerEncoder"
    block: "transformer"
    dim: *hidden_size
    num_heads: 4
    num_hidden: 2048
    num_blocks: 12
    dropout_rate: *dropout_rate
    attention_dropout_rate: 0.0
    norm_type: "prenorm"
    activation_name: "relu"
    concat_after: false
    # chunk_size: 0
    # use_dynamic_chunk: false

  loss:
    # ctc_loss_weight: 0.1
    # ctc_loss: 
    #   module: "mountaintop.layers.loss:CTC"
    #   in_dim: *hidden_size
    #   num_classes: *vocab_size
    #   dropout_rate: *dropout_rate
    #   reduction: "mean"
    
    ce_loss_weight: 1.0
    ce_loss: 
      module: "mountaintop.layers.loss:LabelSmoothingCeLoss"
      smoothing: 0.0
      reduction: "mean"


data_fetcher:
  module: "mountaintop.dataset.vad.loader:VadDatasetLoader"
  tokenizer:
    vocab_path: "data/words.txt"
  filter:
    min_length: 50
    max_length: 2500
  feature:
    type: "fbank"
    dim: 80
    dither: 1.0
  feature:
    type: "fbank"
    dim: 80
    dither: 1.0
  specaug:
    num_t_mask: 2
    num_f_mask: 2
    max_t: 50
    max_f: 10
    # max_w: 80
  # specsub:
  #   max_t: 20
  #   num_t_sub: 3
  batch:
    # batch_type: "dynamic"
    # capcity: 9600
    batch_type: "static"
    capcity: 24

dataset: &dataset 
  train: &train_file "data/supervisions_dev.jsonl"
  valid: &valid_file "data/supervisions_dev.jsonl"
  test: &test_file "data/supervisions_test.jsonl"

train:
  optimizer:
    name: "adam"
    lr: 0.002
  warmup:
    warmup_steps: 25000
    last_epoch: -1

  num_epochs: 80
  grad_clip_norm: 5.0

  checkpoint_selector:
    eval_metric: "loss"
    higher_better: false
  log_interval: 100
  grad_accum_steps: 1
  update_scheduler_by_epoch: false
  use_cudnn_benchmark: true
  use_cuda_nonblocking: false
  use_sync_bn: true
  use_horovod: false
  use_amp: false
  use_prefetcher: false
  log_gradient: false
  keep_old_model: true

  model_average: true
  model_average_interval: 100






